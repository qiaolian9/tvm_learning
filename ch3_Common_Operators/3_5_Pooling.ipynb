{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57dadb7e",
   "metadata": {},
   "source": [
    "# Cmpute definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148613ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import d2ltvm\n",
    "import tvm\n",
    "from tvm import te\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370cb43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool(pool_type, c, nh, nw, kh, kw, ph=0, pw=0, sh=1, sw=1):\n",
    "    \"\"\" 2D pooling\n",
    "        pool_type: pooling type, 'max' or 'avg'\n",
    "        c : channels\n",
    "        nh, nw : input width and height\n",
    "        kh, kw : kernel width and height\n",
    "        ph, pw : height and width padding sizes, default 0\n",
    "        sh, sw : height and width strides, default 1\n",
    "    \"\"\"\n",
    "    # reduction axis\n",
    "    rkh = te.reduce_axis((0, kh), name='rkh')\n",
    "    rkw = te.reduce_axis((0, kw), name='rkw')\n",
    "    # output height and weights\n",
    "    X = te.placeholder((c, nh, nw), name='X')\n",
    "    oh = d2ltvm.conv_out_size(nh, kh, ph, sh)\n",
    "    ow = d2ltvm.conv_out_size(nw, kw, pw, sw)\n",
    "    \n",
    "    if pool_type == 'max':\n",
    "        PaddedX = d2ltvm.padding(X, ph, pw, val=te.min_value(X.dtype)) if ph | pw != 0 else X\n",
    "        Y = te.compute((c, oh, ow), \\\n",
    "                      lambda c, i, j: te.max(PaddedX[c, i * sh + rkh, j * sw + rkw],\\\n",
    "                        axis=[rkh, rkw]), tag='pool_max', name='PoolMax')\n",
    "    elif pool_type == 'avg':\n",
    "        PaddedX = d2ltvm.padding(X, ph, pw, val=0) if ph | pw != 0 else X\n",
    "        tSum = te.compute((c, oh, ow), \\\n",
    "                      lambda c, i, j: te.sum(PaddedX[c, i * sh + rkh, j * sw + rkw], \\\n",
    "                        axis=[rkh, rkw]), tag='pool_avg1', name='PoolSum')\n",
    "        Y = te.compute((c, oh, ow), \\\n",
    "                      lambda c, i, j: tSum[c, i, j] / (kh * kw), \\\n",
    "                        tag='pool_avg2', name='PoolAvg')\n",
    "    else:\n",
    "        raise ValueError(\"pool type should be 'avg' or 'max'\")\n",
    "    return X, Y, PaddedX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d2a2722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(X_1: handle, PoolMax_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {X: Buffer(X_2: Pointer(float32), float32, [576], []),\n",
      "             PoolMax: Buffer(PoolMax_2: Pointer(float32), float32, [576], [])}\n",
      "  buffer_map = {X_1: X, PoolMax_1: PoolMax}\n",
      "  preflattened_buffer_map = {X_1: X_3: Buffer(X_2, float32, [4, 12, 12], []), PoolMax_1: PoolMax_3: Buffer(PoolMax_2, float32, [4, 12, 12], [])} {\n",
      "  allocate(PaddedX: Pointer(global float32), float32, [784]), storage_scope = global {\n",
      "    for (i0: int32, 0, 4) {\n",
      "      for (i1: int32, 0, 14) {\n",
      "        for (i2: int32, 0, 14) {\n",
      "          PaddedX_1: Buffer(PaddedX, float32, [784], [])[(((i0*196) + (i1*14)) + i2)] = @tir.if_then_else(((((i1 < 1) || (13 <= i1)) || (i2 < 1)) || (13 <= i2)), -3.40282e+38f32, X[((((i0*144) + (i1*12)) + i2) - 13)], dtype=float32)\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (c: int32, 0, 4) {\n",
      "      for (i: int32, 0, 12) {\n",
      "        for (j: int32, 0, 12) {\n",
      "          PoolMax[(((c*144) + (i*12)) + j)] = -3.40282e+38f32\n",
      "          for (rkh: int32, 0, 3) {\n",
      "            for (rkw: int32, 0, 3) {\n",
      "              let cse_var_1: int32 = (((c*144) + (i*12)) + j)\n",
      "              PoolMax[cse_var_1] = max(PoolMax[cse_var_1], PaddedX_1[(((((c*196) + (i*14)) + (rkh*14)) + j) + rkw)])\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c, n, k, p, s = 4, 12, 3, 1, 1\n",
    "X, Y, _ = pool('max', c, n, n, k, k, p, p, s, s)\n",
    "sch = te.create_schedule(Y.op)\n",
    "mod = tvm.build(sch, [X, Y])\n",
    "print(tvm.lower(sch, [X, Y], simple_mode=True))\n",
    "data, _, out_max = d2ltvm.get_conv_data(c, c, n, k, p, s, tvm.nd.array)\n",
    "mod(data, out_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bf83bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(X_1: handle, PoolAvg_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {X: Buffer(X_2: Pointer(float32), float32, [576], []),\n",
      "             PoolAvg: Buffer(PoolAvg_2: Pointer(float32), float32, [576], [])}\n",
      "  buffer_map = {X_1: X, PoolAvg_1: PoolAvg}\n",
      "  preflattened_buffer_map = {X_1: X_3: Buffer(X_2, float32, [4, 12, 12], []), PoolAvg_1: PoolAvg_3: Buffer(PoolAvg_2, float32, [4, 12, 12], [])} {\n",
      "  allocate(PaddedX: Pointer(global float32), float32, [784]), storage_scope = global;\n",
      "  allocate(PoolSum: Pointer(global float32), float32, [576]), storage_scope = global {\n",
      "    for (i0: int32, 0, 4) {\n",
      "      for (i1: int32, 0, 14) {\n",
      "        for (i2: int32, 0, 14) {\n",
      "          PaddedX_1: Buffer(PaddedX, float32, [784], [])[(((i0*196) + (i1*14)) + i2)] = @tir.if_then_else(((((i1 < 1) || (13 <= i1)) || (i2 < 1)) || (13 <= i2)), 0f32, X[((((i0*144) + (i1*12)) + i2) - 13)], dtype=float32)\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (c: int32, 0, 4) {\n",
      "      for (i: int32, 0, 12) {\n",
      "        for (j: int32, 0, 12) {\n",
      "          PoolSum_1: Buffer(PoolSum, float32, [576], [])[(((c*144) + (i*12)) + j)] = 0f32\n",
      "          for (rkh: int32, 0, 3) {\n",
      "            for (rkw: int32, 0, 3) {\n",
      "              let cse_var_1: int32 = (((c*144) + (i*12)) + j)\n",
      "              PoolSum_1[cse_var_1] = (PoolSum_1[cse_var_1] + PaddedX_1[(((((c*196) + (i*14)) + (rkh*14)) + j) + rkw)])\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (c_1: int32, 0, 4) {\n",
      "      for (i_1: int32, 0, 12) {\n",
      "        for (j_1: int32, 0, 12) {\n",
      "          let cse_var_2: int32 = (((c_1*144) + (i_1*12)) + j_1)\n",
      "          PoolAvg[cse_var_2] = (PoolSum_1[cse_var_2]*0.111111f32)\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y, _ = pool('avg', c, n, n, k, k, p, p, s, s)\n",
    "sch = te.create_schedule(Y.op)\n",
    "mod = tvm.build(sch, [X, Y])\n",
    "print(tvm.lower(sch, [X, Y], simple_mode=True))\n",
    "data, _, out_avg = d2ltvm.get_conv_data(c, c, n, k, p, s, tvm.nd.array)\n",
    "mod(data, out_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ecf2e",
   "metadata": {},
   "source": [
    "# Torch Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d9664a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pool_data_torch(c, n, k, p, s, ctx='cpu'):\n",
    "    device = torch.device(ctx)\n",
    "    data, _, _ = d2ltvm.get_conv_data(c, c, n, k, p, s, lambda x: torch.tensor(x, device=device))\n",
    "    data = data[None, ...]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec6e5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_torch(pool_type, data, k, p, s):\n",
    "    if pool_type == 'avg':\n",
    "        return torch.nn.functional.avg_pool2d(data, k, s, p)\n",
    "    elif pool_type == 'max':\n",
    "        return torch.nn.functional.max_pool2d(data, k, s, p)\n",
    "    else:\n",
    "        raise ValueError(\"pool type should be 'avg' or 'max'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d570bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_pool_data_torch(c, n, k, p, s)\n",
    "out_max_torch = pool_torch('max', data, k, p, s)\n",
    "data = get_pool_data_torch(c, n, k, p, s)\n",
    "out_avg_torch = pool_torch('avg', data, k, p, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a52a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(out_max.asnumpy(), out_max_torch[0].numpy(), atol=1e-5)\n",
    "np.testing.assert_allclose(out_avg.asnumpy(), out_avg_torch[0].numpy(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0248cb83",
   "metadata": {},
   "source": [
    "# Summary\n",
    "1.2D pooling handles the data in the similar way as 2D convolution, but the computation itself is much lighter.(IO)\\\n",
    "2.We can define max pooling and avg pooling easily using TVM expressions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
