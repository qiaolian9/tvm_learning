{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa515208",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b781759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successd...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import d2ltvm\n",
    "import tvm\n",
    "from tvm import te\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63284b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(X, ph, pw, val=0):\n",
    "    \"\"\" Pad X with the given value in 2-D\n",
    "        ph, pw : height and width padding\n",
    "        val : padding value, default 0\n",
    "    \"\"\"\n",
    "    assert len(X.shape) >= 2\n",
    "    nh, nw = X.shape[-2], X.shape[-1]\n",
    "    return te.compute((*X.shape[0:-2], nh + 2 * ph, nw + 2 * pw), \n",
    "                      lambda *i: te.if_then_else(\n",
    "                          te.any(i[-2] < ph, i[-2] >= ph + nh, i[-1] < pw, i[-1] >= pw + nw), \n",
    "                          val, X[i[:-2] + (i[-2] - ph, i[-1] - pw)]),\n",
    "                     name='PaddedX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41c6cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[<span style=\"color: #008000\">24</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>], PaddedX: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>handle) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> <span style=\"color: #008000; font-weight: bold\">None</span>:\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;from_legacy_te_schedule&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>, <span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        ph <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>var(<span style=\"color: #BA2121\">&quot;int32&quot;</span>)\n",
       "        pw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>var(<span style=\"color: #BA2121\">&quot;int32&quot;</span>)\n",
       "        PaddedX_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>match_buffer(PaddedX, [<span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> (<span style=\"color: #008000\">3</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> ph) <span style=\"color: #AA22FF; font-weight: bold\">*</span> (<span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> pw)], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>preflattened_buffer(A, [<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>, data<span style=\"color: #AA22FF; font-weight: bold\">=</span>A<span style=\"color: #AA22FF; font-weight: bold\">.</span>data)\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>preflattened_buffer(PaddedX_1, [<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">3</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> ph, <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> pw], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>, data<span style=\"color: #AA22FF; font-weight: bold\">=</span>PaddedX_1<span style=\"color: #AA22FF; font-weight: bold\">.</span>data)\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, i2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">2</span>, ph <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">3</span>, pw <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">4</span>):\n",
       "            PaddedX_1[(i0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> (ph <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">3</span>) <span style=\"color: #AA22FF; font-weight: bold\">+</span> i1) <span style=\"color: #AA22FF; font-weight: bold\">*</span> (pw <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">4</span>) <span style=\"color: #AA22FF; font-weight: bold\">+</span> i2] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(i1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> ph <span style=\"color: #008000; font-weight: bold\">or</span> ph <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">3</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i1 <span style=\"color: #008000; font-weight: bold\">or</span> i2 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> pw <span style=\"color: #008000; font-weight: bold\">or</span> pw <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> i2, T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>), A[i0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">12</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i2 <span style=\"color: #AA22FF; font-weight: bold\">-</span> pw <span style=\"color: #AA22FF; font-weight: bold\">-</span> ph <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = te.placeholder((2,3,4), name='A')\n",
    "ph = te.var(name='ph')\n",
    "pw = te.var(name='pw')\n",
    "B = padding(A, ph, pw)\n",
    "s = te.create_schedule(B.op)\n",
    "tvm.lower(s, [A, B]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db408229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 1. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "mod = tvm.build(s, [A, B, ph, pw])\n",
    "ph_1 = 1\n",
    "pw_1 = 1\n",
    "a = tvm.nd.array(np.ones((2,3,4)).astype('float32'))\n",
    "b = tvm.nd.array(np.empty((2,5,6)).astype('float32'))\n",
    "mod(a, b, ph_1, pw_1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a753d524",
   "metadata": {},
   "source": [
    "# Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8542e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_out_size(n, k, p, s):\n",
    "    \"\"\" Compute the output size by given input size n (width or height),\n",
    "        kernel size k, padding p, and stride s\n",
    "        Return output size (width or height)\n",
    "    \"\"\"\n",
    "    return (n + 2 * p - k) // s + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95056943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(oc, ic, nh, nw, kh, kw, ph=0, pw=0, sh=1, sw=1):\n",
    "    \"\"\" Convolution\n",
    "        oc, ic : output and input channels\n",
    "        nh, nw : input width and height\n",
    "        kh, kw : kernel width and height\n",
    "        ph, pw : height and width padding sizes, default 0\n",
    "        sh, sw : height and width strides, default 1\n",
    "    \"\"\"\n",
    "    ric = te.reduce_axis((0, ic), name='ric')\n",
    "    rkh = te.reduce_axis((0, kh), name='rkh')\n",
    "    rkw = te.reduce_axis((0, kw), name='rkw')\n",
    "    \n",
    "    oh = conv_out_size(nh, kh, ph, sh)\n",
    "    ow = conv_out_size(nw, kw, pw, sw)\n",
    "    \n",
    "    # pad X and then compute Y    \n",
    "    X = te.placeholder((ic, nh, nw), name='X')\n",
    "    K = te.placeholder((oc, ic, kh, kw), name='K')\n",
    "    \n",
    "    PaddedX = padding(X, ph, pw) if ph * pw != 0 else X\n",
    "    Y = te.compute((oc, oh, ow), \n",
    "            lambda c, i, j: te.sum(PaddedX[ric, i * sh + rkh, j * sw + rkw] * K[c, ric, rkh, rkw], \n",
    "                    axis=[ric, rkh, rkw]), name='Y')\n",
    "    return X, K, Y, PaddedX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea6243e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_data(oc, ic, n, k, p, s=1, constructor=None):\n",
    "    \"\"\" Return random 3-D data tensor, 3-D kernel tenor and empty 3-D output\n",
    "        tensor with the shapes specified by input arguments.\n",
    "        oc, ic : output and input channels\n",
    "        n : input width and height\n",
    "        k : kernel width and height\n",
    "        p : padding size, default 0\n",
    "        s : stride, default 1\n",
    "        constructor : user-defined tensor constructor\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    data = np.random.normal(size=(ic, n, n)).astype('float32')\n",
    "    weight = np.random.normal(size=(oc, ic, k, k)).astype('float32')\n",
    "    on = conv_out_size(n, k, p, s)\n",
    "    out = np.empty(shape=(oc, on, on)).astype('float32')\n",
    "    if constructor is not None:\n",
    "        data, weight, out = [constructor(i) for i in (data, weight, out)]\n",
    "    \n",
    "    return data, weight, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f2373c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(X_1: handle, K_1: handle, Y_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {X: Buffer(X_2: Pointer(float32), float32, [864], []),\n",
      "             K: Buffer(K_2: Pointer(float32), float32, [216], []),\n",
      "             Y: Buffer(Y_2: Pointer(float32), float32, [576], [])}\n",
      "  buffer_map = {X_1: X, K_1: K, Y_1: Y}\n",
      "  preflattened_buffer_map = {X_1: X_3: Buffer(X_2, float32, [6, 12, 12], []), K_1: K_3: Buffer(K_2, float32, [4, 6, 3, 3], []), Y_1: Y_3: Buffer(Y_2, float32, [4, 12, 12], [])} {\n",
      "  allocate(PaddedX: Pointer(global float32), float32, [1176]), storage_scope = global {\n",
      "    for (i0: int32, 0, 6) {\n",
      "      for (i1: int32, 0, 14) {\n",
      "        for (i2: int32, 0, 14) {\n",
      "          PaddedX_1: Buffer(PaddedX, float32, [1176], [])[(((i0*196) + (i1*14)) + i2)] = @tir.if_then_else(((((i1 < 1) || (13 <= i1)) || (i2 < 1)) || (13 <= i2)), 0f32, X[((((i0*144) + (i1*12)) + i2) - 13)], dtype=float32)\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (c: int32, 0, 4) {\n",
      "      for (i: int32, 0, 12) {\n",
      "        for (j: int32, 0, 12) {\n",
      "          Y[(((c*144) + (i*12)) + j)] = 0f32\n",
      "          for (ric: int32, 0, 6) {\n",
      "            for (rkh: int32, 0, 3) {\n",
      "              for (rkw: int32, 0, 3) {\n",
      "                let cse_var_1: int32 = (((c*144) + (i*12)) + j)\n",
      "                Y[cse_var_1] = (Y[cse_var_1] + (PaddedX_1[(((((ric*196) + (i*14)) + (rkh*14)) + j) + rkw)]*K[((((c*54) + (ric*9)) + (rkh*3)) + rkw)]))\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oc, ic, n, k, p, s = 4, 6, 12, 3, 1, 1\n",
    "X, K, Y, _ = conv(oc, ic, n, n, k, k, p, p, s, s)\n",
    "sch = te.create_schedule(Y.op)\n",
    "mod = tvm.build(sch, [X, K, Y])\n",
    "print(tvm.lower(sch, [X, K, Y], simple_mode=True))\n",
    "data, weight, out = get_conv_data(oc, ic, n, k, p, s, constructor=tvm.nd.array)\n",
    "mod(data, weight, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a87c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_conv_data_torch(oc, ic, n, k, p, s, ctx='cpu'):\n",
    "    ctx = torch.device(ctx)\n",
    "    data, weight, out = get_conv_data(oc, ic, n, k, p, s, lambda x: torch.tensor(x, device=ctx))\n",
    "    data, out = data[None, ...], out[None, ...]\n",
    "    bias = torch.zeros(out.shape[1], device=ctx)\n",
    "    return data, weight, bias, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cb047fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_torch(data, weight, bias, k, p, s):\n",
    "    return torch.nn.functional.conv2d(data, weight, bias=bias, stride=s, padding=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d72a66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "conv1 = nn.Conv2d(oc, ic, k)\n",
    "data1, weight1, bias1, out_torch1 = get_conv_data_torch(oc, ic, n, k, p, s)\n",
    "out_torch = conv_torch(data1, weight1, bias1, k, p, s)\n",
    "np.testing.assert_allclose(out.asnumpy(), out_torch[0].numpy(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2f95e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "1.We can express the computation of 2-D convolution in TVM in a fairly easy way. \\\n",
    "2.Deep learning workloads normally operate 2-D convolution on 4-D data tensors and kernel tensors. \\\n",
    "3.The naive 2-D convolution is a 6-level nested for loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
