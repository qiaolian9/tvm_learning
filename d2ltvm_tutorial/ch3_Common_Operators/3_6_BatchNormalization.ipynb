{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45067f5",
   "metadata": {},
   "source": [
    "# Compute definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc532af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successd...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import d2ltvm\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a08b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import topi\n",
    "def batch_norm(c, n, eps=1e-5):\n",
    "    \"\"\" batch normalization\n",
    "        c : channels\n",
    "        N : input width and height\n",
    "        eps : small positive value to prevent divide 0\n",
    "    \"\"\"\n",
    "    X = te.placeholder((c, n, n), name='X')\n",
    "    Mean = te.placeholder((c, 1, 1), name='Mean')\n",
    "    Var = te.placeholder((c, 1, 1), name='Var')\n",
    "    Gamma = te.placeholder((c, 1, 1), name='Gamma')\n",
    "    Beta = te.placeholder((c, 1, 1), name='Beta')\n",
    "    C1 = X - Mean\n",
    "    C2 = topi.sqrt(Var + eps)\n",
    "    Y = C1 / C2 * Gamma + Beta\n",
    "    return X, Mean, Var, Gamma, Beta, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "059fe4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(X_1: handle, Mean_1: handle, Var_1: handle, Gamma_1: handle, Beta_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {X: Buffer(X_2: Pointer(float32), float32, [21952], []),\n",
      "             Mean: Buffer(Mean_2: Pointer(float32), float32, [28], []),\n",
      "             Var: Buffer(Var_2: Pointer(float32), float32, [28], []),\n",
      "             Gamma: Buffer(Gamma_2: Pointer(float32), float32, [28], []),\n",
      "             Beta: Buffer(Beta_2: Pointer(float32), float32, [28], [])}\n",
      "  buffer_map = {X_1: X, Mean_1: Mean, Var_1: Var, Gamma_1: Gamma, Beta_1: Beta}\n",
      "  preflattened_buffer_map = {Var_1: Var_3: Buffer(Var_2, float32, [28, 1, 1], []), Gamma_1: Gamma_3: Buffer(Gamma_2, float32, [28, 1, 1], []), Beta_1: Beta_3: Buffer(Beta_2, float32, [28, 1, 1], []), Mean_1: Mean_3: Buffer(Mean_2, float32, [28, 1, 1], []), X_1: X_3: Buffer(X_2, float32, [28, 28, 28], [])} {\n",
      "  allocate(T_subtract: Pointer(global float32), float32, [21952]), storage_scope = global;\n",
      "  allocate(T_add: Pointer(global float32), float32, [28]), storage_scope = global {\n",
      "    for (ax0: int32, 0, 28) {\n",
      "      for (ax1: int32, 0, 28) {\n",
      "        for (ax2: int32, 0, 28) {\n",
      "          let cse_var_1: int32 = (((ax0*784) + (ax1*28)) + ax2)\n",
      "          T_subtract_1: Buffer(T_subtract, float32, [21952], [])[cse_var_1] = (X[cse_var_1] - Mean[ax0])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (ax0_1: int32, 0, 28) {\n",
      "      T_add_1: Buffer(T_add, float32, [28], [])[ax0_1] = (Var[ax0_1] + 1e-05f32)\n",
      "    }\n",
      "    for (i0: int32, 0, 28) {\n",
      "      T_add_2: Buffer(T_add, float32, [28], [])[i0] = @tir.sqrt(T_add_1[i0], dtype=float32)\n",
      "    }\n",
      "    for (ax0_2: int32, 0, 28) {\n",
      "      for (ax1_1: int32, 0, 28) {\n",
      "        for (ax2_1: int32, 0, 28) {\n",
      "          let cse_var_2: int32 = (((ax0_2*784) + (ax1_1*28)) + ax2_1)\n",
      "          T_subtract_2: Buffer(T_subtract, float32, [21952], [])[cse_var_2] = (T_subtract_1[cse_var_2] / T_add_2[ax0_2])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (ax0_3: int32, 0, 28) {\n",
      "      for (ax1_2: int32, 0, 28) {\n",
      "        for (ax2_2: int32, 0, 28) {\n",
      "          let cse_var_3: int32 = (((ax0_3*784) + (ax1_2*28)) + ax2_2)\n",
      "          T_subtract_3: Buffer(T_subtract, float32, [21952], [])[cse_var_3] = (T_subtract_2[cse_var_3]*Gamma[ax0_3])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (ax0_4: int32, 0, 28) {\n",
      "      for (ax1_3: int32, 0, 28) {\n",
      "        for (ax2_3: int32, 0, 28) {\n",
      "          let cse_var_4: int32 = (((ax0_4*784) + (ax1_3*28)) + ax2_3)\n",
      "          T_subtract_4: Buffer(T_subtract, float32, [21952], [])[cse_var_4] = (T_subtract_3[cse_var_4] + Beta[ax0_4])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c, n = 28, 28\n",
    "X, Mean, Var, Gamma, Beta, Y = batch_norm(c, n)\n",
    "sch = te.create_schedule(Y.op)\n",
    "mod = tvm.build(sch, [X, Mean, Var, Gamma, Beta, Y])\n",
    "print(tvm.lower(sch, [X, Mean, Var, Gamma, Beta], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "610dcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_data(c, n, constructor=None):\n",
    "    \"\"\" Return the batch norm data, mean, variance, gamma and beta tensors.\n",
    "        Also return the empty tensor for output.\n",
    "        c : channels\n",
    "        n : input width and height\n",
    "        constructor : user-defined tensor constructor\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    data = np.random.normal(size=(c, n, c)).astype('float32')\n",
    "    mean = np.random.normal(size=(c, 1, 1)).astype('float32')\n",
    "    \n",
    "    var = np.random.normal(size=(c, 1, 1)).astype('float32')\n",
    "    var = np.absolute(var)\n",
    "    \n",
    "    gamma = np.random.normal(size=(c, 1, 1)).astype('float32')\n",
    "    beta = np.random.normal(size=(c, 1, 1)).astype('float32')\n",
    "    out = np.empty(shape=(c, n, n)).astype('float32')\n",
    "    \n",
    "    if constructor is not None:\n",
    "        data, mean, var, gamma, beta, out = \\\n",
    "            [constructor(x) for x in (data, mean, var, gamma, beta, out)]\n",
    "    \n",
    "    return data, mean, var, gamma, beta, out\n",
    "\n",
    "data, mean, var, gamma, beta, out = get_bn_data(c, n, tvm.nd.array)\n",
    "mod(data, mean, var, gamma, beta, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61688b",
   "metadata": {},
   "source": [
    "# Torch baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7ef3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_bn_data_torch(c, n, ctx='cpu'):\n",
    "    device = torch.device(ctx)\n",
    "    data, mean, var, gamma, beta, _ = get_bn_data(c, n, lambda x: torch.tensor(x, device=device))\n",
    "    data = data[None, ...]\n",
    "    mean, var, gamma, beta = [x[:, 0, 0] for x in (mean, var, gamma, beta)]\n",
    "    return data, mean, var, gamma, beta\n",
    "\n",
    "def batch_norm_torch(data, mean, var, gamma, beta, eps=1e-5):\n",
    "    return torch.nn.functional.batch_norm(data, mean, var, gamma, beta, eps=eps)\n",
    "\n",
    "data, mean, var, gamma, beta = get_bn_data_torch(c, n)\n",
    "out_torch = batch_norm_torch(data, mean, var, gamma, beta)\n",
    "np.testing.assert_allclose(out.asnumpy(), out_torch[0].numpy(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672689a",
   "metadata": {},
   "source": [
    "# Summary\n",
    "1.From the computation perspective, batch_norm is a combination of a number of broadcast and element-wise simple operators, which can be easily attained from TVMâ€™s Tensor OPerator Inventory(TOPI).\\\n",
    "2.In inference, mean and var of batch_norm are pre-defined."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
