{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MLC.mlc as mlc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import MLC.demo.model.resnet as Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = Resnet.resnet18(num_classes=10)\n",
    "# img_np = np.random.rand(1, 1, 384, 384).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.MaxPool2d((3, 3), stride=1)\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3, 1)\n",
    "        self.bn = nn.BatchNorm2d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1,1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x+ x\n",
    "        # x = self.maxpool(x)\n",
    "        x = self.conv1(x)\n",
    "        # x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.avg(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name     target                                                      args        kwargs\n",
      "-------------  -------  ----------------------------------------------------------  ----------  --------\n",
      "placeholder    x        x                                                           ()          {}\n",
      "call_function  add      <built-in function add>                                     (x, x)      {}\n",
      "call_module    conv1    conv1                                                       (add,)      {}\n",
      "call_module    relu     relu                                                        (conv1,)    {}\n",
      "call_module    avg      avg                                                         (relu,)     {}\n",
      "call_function  flatten  <built-in method flatten of type object at 0x7f7363859ec0>  (avg, 1)    {}\n",
      "output         output   output                                                      (flatten,)  {}\n",
      "x x <class 'str'>\n",
      "add <built-in function add> <class 'builtin_function_or_method'>\n",
      "conv1 conv1 <class 'str'>\n",
      "relu relu <class 'str'>\n",
      "avg avg <class 'str'>\n",
      "flatten <built-in method flatten of type object at 0x7f7363859ec0> <class 'builtin_function_or_method'>\n",
      "output output <class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "builtin_function_or_method"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_module = torch.fx.symbolic_trace(resnet())\n",
    "fx_module.graph.print_tabular()\n",
    "for i in fx_module.graph.nodes:\n",
    "    print(i, i.target, type(i.target))\n",
    "\n",
    "type(abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "/staff/qiaoliang/anaconda3/envs/MLC/bin/python -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">384</span>, <span style=\"color: #008000\">384</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">384</span>, <span style=\"color: #008000\">384</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(x, x)\n",
       "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">382</span>, <span style=\"color: #008000\">382</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>conv2d(lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>], strides<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], dilation<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">382</span>, <span style=\"color: #008000\">382</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv1, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
       "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">382</span>, <span style=\"color: #008000\">382</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv2)\n",
       "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>adaptive_avg_pool2d(lv3, output_size<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>)\n",
       "            lv5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>reshape(lv4, (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>))\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv5\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet_fx_module = mlc.from_fx(fx_module, [(1, 1, 384, 384)])\n",
    "resnet_fx_module.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "/staff/qiaoliang/anaconda3/envs/MLC/bin/python -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">384</span>, <span style=\"color: #008000\">384</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">384</span>, <span style=\"color: #008000\">384</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(x, x)\n",
       "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">382</span>, <span style=\"color: #008000\">382</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>conv2d(lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>], strides<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], dilation<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">382</span>, <span style=\"color: #008000\">382</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv1, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
       "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">382</span>, <span style=\"color: #008000\">382</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv2)\n",
       "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>adaptive_avg_pool2d(lv3, output_size<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>)\n",
       "            lv5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>reshape(lv4, (<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>))\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv5\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet_fused = mlc.FuseDenseAddPass()(resnet_fx_module)\n",
    "resnet_fused.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  5: TVMFuncCall\n  4: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::transform::Pass, tvm::IRModule)>::AssignTypedLambda<tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}>(tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}, std::string)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  3: tvm::transform::Pass::operator()(tvm::IRModule) const\n  2: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  1: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .cold]\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 56, in tvm._ffi._cy3.core.tvm_callback\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/ir/transform.py\", line 307, in _pass_func\n    return inst.transform_module(mod, ctx)\n  File \"/staff/qiaoliang/ACSA科研项目/tvm_learning/MLC/mlc/mlc.py\", line 386, in transform_module\n    return LowerToTensorIR(mod, op_map).transform()\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/meta_schedule/utils.py\", line 139, in method\n    return result(*args, **kwargs)\n  File \"/staff/qiaoliang/ACSA科研项目/tvm_learning/MLC/mlc/mlc.py\", line 376, in transform\n    updated_fn = self.visit_expr(func)\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/relax/expr_functor.py\", line 983, in visit_expr\n    return _ffi_api.PyExprMutatorVisitExpr(self._outer(), expr)  # type: ignore\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 331, in tvm._ffi._cy3.core.PackedFuncBase.__call__\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 262, in tvm._ffi._cy3.core.FuncCall\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 251, in tvm._ffi._cy3.core.FuncCall3\n  File \"tvm/_ffi/_cython/./base.pxi\", line 181, in tvm._ffi._cy3.core.CHECK_CALL\n  24: TVMFuncCall\n  23: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::RelayExpr (tvm::relax::PyExprMutator, tvm::RelayExpr const&)>::AssignTypedLambda<tvm::relax::{lambda(tvm::relax::PyExprMutator, tvm::RelayExpr const&)#12}>(tvm::relax::{lambda(tvm::relax::PyExprMutator, tvm::RelayExpr const&)#12}, std::string)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  22: tvm::relax::PyExprMutatorNode::VisitExpr(tvm::RelayExpr const&)\n  21: tvm::NodeFunctor<tvm::RelayExpr (tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)>::operator()(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*) const\n  20: tvm::relax::PyExprMutatorNode::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)#8}::_FUN(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)\n  19: tvm::relax::ExprMutator::VisitExpr_(tvm::relax::FunctionNode const*)\n  18: tvm::relax::ExprMutator::VisitWithNewScope(tvm::RelayExpr const&, tvm::runtime::Optional<tvm::runtime::Array<tvm::relax::Var, void> >)\n  17: tvm::relax::PyExprMutatorNode::VisitExpr(tvm::RelayExpr const&)\n  16: tvm::NodeFunctor<tvm::RelayExpr (tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)>::operator()(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*) const\n  15: tvm::relax::PyExprMutatorNode::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)#10}::_FUN(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)\n  14: tvm::relax::ExprMutator::VisitExpr_(tvm::relax::SeqExprNode const*)\n  13: tvm::relax::PyExprMutatorNode::VisitBindingBlock(tvm::relax::BindingBlock const&)\n  12: tvm::relax::ExprMutator::VisitBindingBlock(tvm::relax::BindingBlock const&)\n  11: tvm::relax::PyExprMutatorNode::VisitBindingBlock_(tvm::relax::DataflowBlockNode const*)\n  10: tvm::relax::ExprMutator::VisitBindingBlock_(tvm::relax::DataflowBlockNode const*)\n  9: tvm::relax::PyExprMutatorNode::VisitBinding(tvm::relax::Binding const&)\n  8: tvm::relax::ExprMutator::VisitBinding(tvm::relax::Binding const&)\n  7: tvm::relax::PyExprMutatorNode::VisitBinding_(tvm::relax::VarBindingNode const*)\n  6: tvm::relax::ExprMutator::VisitBinding_(tvm::relax::VarBindingNode const*)\n  5: tvm::relax::ExprMutator::VisitBinding_(tvm::relax::VarBindingNode const*, tvm::relax::DataflowVarNode const*)\n  4: tvm::relax::PyExprMutatorNode::VisitExpr(tvm::RelayExpr const&)\n  3: tvm::NodeFunctor<tvm::RelayExpr (tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)>::operator()(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*) const\n  2: tvm::relax::PyExprMutatorNode::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)#9}::_FUN(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)\n  1: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::runtime::ObjectRef const&>(tvm::runtime::ObjectRef const&) const\n  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .cold]\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 56, in tvm._ffi._cy3.core.tvm_callback\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/meta_schedule/utils.py\", line 76, in method\n    return getattr(inst, name)(*args, **kwargs)\n  File \"/staff/qiaoliang/ACSA科研项目/tvm_learning/MLC/mlc/mlc.py\", line 369, in visit_call_\n    return self.op_map[call.op](self.builder_, call, self.lookup_binding)\n  File \"/staff/qiaoliang/ACSA科研项目/tvm_learning/MLC/mlc/mlc.py\", line 336, in map_adaptiveAvgPool2d_te\n    output_size, layout, out_layout = call.attrs\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/ir/attrs.py\", line 93, in __getitem__\n    return self.__getattr__(item)\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/runtime/object.py\", line 76, in __getattr__\n    return _ffi_node_api.NodeGetAttr(self, name)\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 331, in tvm._ffi._cy3.core.PackedFuncBase.__call__\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 262, in tvm._ffi._cy3.core.FuncCall\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 251, in tvm._ffi._cy3.core.FuncCall3\n  File \"tvm/_ffi/_cython/./base.pxi\", line 181, in tvm._ffi._cy3.core.CHECK_CALL\n  2: TVMFuncCall\n  1: tvm::NodeGetAttr(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  0: tvm::runtime::TVMArgValue::operator std::string() const\n  File \"/workspace/tvm/include/tvm/runtime/packed_func.h\", line 681\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (IsObjectRef<tvm::runtime::String>()) is false: Could not convert TVM object of type runtime.Object to a string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/staff/qiaoliang/ACSA科研项目/tvm_learning/MLC/demo/demo2.ipynb 单元格 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsnode2/staff/qiaoliang/ACSA%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE/tvm_learning/MLC/demo/demo2.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m lowresnet \u001b[39m=\u001b[39m mlc\u001b[39m.\u001b[39;49mLowerToTensorIRPass()(resnet_fused)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsnode2/staff/qiaoliang/ACSA%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE/tvm_learning/MLC/demo/demo2.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m lowresnet\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/ir/transform.py:238\u001b[0m, in \u001b[0;36mPass.__call__\u001b[0;34m(self, mod)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, mod):\n\u001b[1;32m    225\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Execute the pass. Note that for sequential pass, the dependency among\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m    different passes will be resolved in the backend.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39m        The updated module after applying this pass.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mreturn\u001b[39;00m _ffi_transform_api\u001b[39m.\u001b[39;49mRunPass(\u001b[39mself\u001b[39;49m, mod)\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi:331\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.PackedFuncBase.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi:262\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi:251\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.FuncCall3\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./base.pxi:181\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.CHECK_CALL\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  5: TVMFuncCall\n  4: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::transform::Pass, tvm::IRModule)>::AssignTypedLambda<tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}>(tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}, std::string)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  3: tvm::transform::Pass::operator()(tvm::IRModule) const\n  2: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  1: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .cold]\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 56, in tvm._ffi._cy3.core.tvm_callback\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/ir/transform.py\", line 307, in _pass_func\n    return inst.transform_module(mod, ctx)\n  File \"/staff/qiaoliang/ACSA科研项目/tvm_learning/MLC/mlc/mlc.py\", line 386, in transform_module\n    return LowerToTensorIR(mod, op_map).transform()\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/meta_schedule/utils.py\", line 139, in method\n    return result(*args, **kwargs)\n  File \"/staff/qiaoliang/ACSA科研项目/tvm_learning/MLC/mlc/mlc.py\", line 376, in transform\n    updated_fn = self.visit_expr(func)\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/relax/expr_functor.py\", line 983, in visit_expr\n    return _ffi_api.PyExprMutatorVisitExpr(self._outer(), expr)  # type: ignore\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 331, in tvm._ffi._cy3.core.PackedFuncBase.__call__\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 262, in tvm._ffi._cy3.core.FuncCall\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 251, in tvm._ffi._cy3.core.FuncCall3\n  File \"tvm/_ffi/_cython/./base.pxi\", line 181, in tvm._ffi._cy3.core.CHECK_CALL\n  24: TVMFuncCall\n  23: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::RelayExpr (tvm::relax::PyExprMutator, tvm::RelayExpr const&)>::AssignTypedLambda<tvm::relax::{lambda(tvm::relax::PyExprMutator, tvm::RelayExpr const&)#12}>(tvm::relax::{lambda(tvm::relax::PyExprMutator, tvm::RelayExpr const&)#12}, std::string)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  22: tvm::relax::PyExprMutatorNode::VisitExpr(tvm::RelayExpr const&)\n  21: tvm::NodeFunctor<tvm::RelayExpr (tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)>::operator()(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*) const\n  20: tvm::relax::PyExprMutatorNode::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)#8}::_FUN(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)\n  19: tvm::relax::ExprMutator::VisitExpr_(tvm::relax::FunctionNode const*)\n  18: tvm::relax::ExprMutator::VisitWithNewScope(tvm::RelayExpr const&, tvm::runtime::Optional<tvm::runtime::Array<tvm::relax::Var, void> >)\n  17: tvm::relax::PyExprMutatorNode::VisitExpr(tvm::RelayExpr const&)\n  16: tvm::NodeFunctor<tvm::RelayExpr (tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)>::operator()(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*) const\n  15: tvm::relax::PyExprMutatorNode::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)#10}::_FUN(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)\n  14: tvm::relax::ExprMutator::VisitExpr_(tvm::relax::SeqExprNode const*)\n  13: tvm::relax::PyExprMutatorNode::VisitBindingBlock(tvm::relax::BindingBlock const&)\n  12: tvm::relax::ExprMutator::VisitBindingBlock(tvm::relax::BindingBlock const&)\n  11: tvm::relax::PyExprMutatorNode::VisitBindingBlock_(tvm::relax::DataflowBlockNode const*)\n  10: tvm::relax::ExprMutator::VisitBindingBlock_(tvm::relax::DataflowBlockNode const*)\n  9: tvm::relax::PyExprMutatorNode::VisitBinding(tvm::relax::Binding const&)\n  8: tvm::relax::ExprMutator::VisitBinding(tvm::relax::Binding const&)\n  7: tvm::relax::PyExprMutatorNode::VisitBinding_(tvm::relax::VarBindingNode const*)\n  6: tvm::relax::ExprMutator::VisitBinding_(tvm::relax::VarBindingNode const*)\n  5: tvm::relax::ExprMutator::VisitBinding_(tvm::relax::VarBindingNode const*, tvm::relax::DataflowVarNode const*)\n  4: tvm::relax::PyExprMutatorNode::VisitExpr(tvm::RelayExpr const&)\n  3: tvm::NodeFunctor<tvm::RelayExpr (tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)>::operator()(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*) const\n  2: tvm::relax::PyExprMutatorNode::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)#9}::_FUN(tvm::runtime::ObjectRef const&, tvm::relax::PyExprMutatorNode*)\n  1: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::runtime::ObjectRef const&>(tvm::runtime::ObjectRef const&) const\n  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .cold]\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 56, in tvm._ffi._cy3.core.tvm_callback\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/meta_schedule/utils.py\", line 76, in method\n    return getattr(inst, name)(*args, **kwargs)\n  File \"/staff/qiaoliang/ACSA科研项目/tvm_learning/MLC/mlc/mlc.py\", line 369, in visit_call_\n    return self.op_map[call.op](self.builder_, call, self.lookup_binding)\n  File \"/staff/qiaoliang/ACSA科研项目/tvm_learning/MLC/mlc/mlc.py\", line 336, in map_adaptiveAvgPool2d_te\n    output_size, layout, out_layout = call.attrs\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/ir/attrs.py\", line 93, in __getitem__\n    return self.__getattr__(item)\n  File \"/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/runtime/object.py\", line 76, in __getattr__\n    return _ffi_node_api.NodeGetAttr(self, name)\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 331, in tvm._ffi._cy3.core.PackedFuncBase.__call__\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 262, in tvm._ffi._cy3.core.FuncCall\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 251, in tvm._ffi._cy3.core.FuncCall3\n  File \"tvm/_ffi/_cython/./base.pxi\", line 181, in tvm._ffi._cy3.core.CHECK_CALL\n  2: TVMFuncCall\n  1: tvm::NodeGetAttr(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  0: tvm::runtime::TVMArgValue::operator std::string() const\n  File \"/workspace/tvm/include/tvm/runtime/packed_func.h\", line 681\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (IsObjectRef<tvm::runtime::String>()) is false: Could not convert TVM object of type runtime.Object to a string."
     ]
    }
   ],
   "source": [
    "lowresnet = mlc.LowerToTensorIRPass()(resnet_fused)\n",
    "lowresnet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Op(relax.nn.adaptive_avg_pool2d)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tvm\n",
    "tvm.ir.Op.get('relax.nn.adaptive_avg_pool2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df7b789d6764113f3eb4ff8e192e7912fbf893c46539f75332048503ce5ba603"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
