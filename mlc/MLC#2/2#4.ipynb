{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorIR: 张量程序抽象案例研究"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 序言\n",
    "1. 什么是表示张量函数可能的抽象？\n",
    "2. 什么是张量函数之间可能的变换？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3 TensorIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import tir as T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量程序抽象的主要目的是表示循环和相关的硬件加速选择，如多线程、特殊硬件指令的使用和内存访问"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& Y_{i,j} = \\sum_{k}{A_{i,k}\\times{B_{k,j}}} \\\\\n",
    "& C_{i,j} = Relu(Y_{i,j}) = max(Y_{i,j},0)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy version\n",
    "dtype = 'float32'\n",
    "a_np = np.random.rand(128, 128).astype(dtype)\n",
    "b_np = np.random.rand(128, 128).astype(dtype)\n",
    "# a @ b is equivalent to np.matmul(a,b)\n",
    "c_mm_relu = np.maximum(a_np @ b_np, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "低级 NumPy\n",
    "1. 使用循环而不是数组函数来展示可能的循环计算\n",
    "2. 通过 numpy.empty 显式地分配数组并传递它们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low level numpy version\n",
    "def lnumpy_mm_relu(A: np.array, B: np.array, C: np.array):           # 多维数组（缓冲区）：input & output & intermediate results\n",
    "    Y = np.empty((128, 128), dtype='float32')\n",
    "    for i in range(128):                                             # 循环嵌套：loop nests -> drive iteration\n",
    "        for j in range(128):\n",
    "            for k in range(128):\n",
    "                if k == 0:\n",
    "                    Y[i, j] = 0\n",
    "                Y[i, j] = Y[i, j] + A[i, k] * B[k, j]                # 计算定义：Computation\n",
    "    \n",
    "    for i in range(128):\n",
    "        for j in range(128):\n",
    "            C[i, j] = max(Y[i, j], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_np = np.empty((128, 128), dtype=dtype)\n",
    "lnumpy_mm_relu(a_np, b_np, c_np)\n",
    "np.testing.assert_allclose(c_np, c_mm_relu, rtol=1e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorIR: TVMScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModel():\n",
    "    @T.prim_func\n",
    "    def mm_relu(A: T.Buffer[(128, 128), 'float32'],\n",
    "                B: T.Buffer[(128, 128), 'float32'],\n",
    "                C: T.Buffer[(128, 128), 'float32']):\n",
    "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
    "        Y = T.alloc_buffer((128, 128), dtype='float32')\n",
    "        for i, j, k in T.grid(128, 128, 128):\n",
    "            with T.block(\"Y\"):\n",
    "                vi = T.axis.spatial(128, i)\n",
    "                vj = T.axis.spatial(128, j)\n",
    "                vk = T.axis.reduce(128, k)\n",
    "                with T.init():\n",
    "                    Y[vi, vj] = T.float32(0)\n",
    "                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
    "        \n",
    "        for i, j in T.grid(128, 128):\n",
    "            with T.block(\"C\"):\n",
    "                vi = T.axis.spatial(128, i)\n",
    "                vj = T.axis.spatial(128, j)\n",
    "                C[vi, vj] = T.max(Y[vi, vj], T.float32(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3.1 Multi-dimensional buffers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**多维数组**：函数参数 & 缓冲区 \\\n",
    "1.input & output\n",
    "```python\n",
    "# TensorIR\n",
    "def mm_relu(A: T.Buffer[(128, 128), dtype='float32'],\n",
    "            B: T.Buffer[(128, 128), dtype='float32'],\n",
    "            C: T.Buffer[(128, 128), dtype='float32']):\n",
    "    pass\n",
    "```\n",
    "```python\n",
    "# numpy\n",
    "def lnumpy_mm_relu(A: np.array, B: np.array, C: np.array):\n",
    "    pass\n",
    "```\n",
    "2.intermediate results\n",
    "```python\n",
    "# TensorIR\n",
    "Y = T.alloc_buffer((128, 128), dtype='float32')\n",
    "```\n",
    "```python\n",
    "# numpy\n",
    "Y = np.empty((128, 128), dtype='float32')\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3.2 For: Loop iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TensorIR\n",
    "for i, j, k in T.grid(128, 128, 128):    # TensorIR语法糖\n",
    "    pass\n",
    "```\n",
    "```python\n",
    "# numpy\n",
    "for i in range(128):\n",
    "    for j in range(128):\n",
    "        for k in range(128):\n",
    "            pass\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3.3 Computation Block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TensorIR\n",
    "with T.block(\"Y\"):\n",
    "    vi = T.axis.spatial(128, i)\n",
    "    vj = T.axis.spatial(128, j)\n",
    "    vk = T.axis.reduce(128, k)\n",
    "    with T.init():\n",
    "        Y[vi, vj] = T.float32(0)\n",
    "    Y[vi, vj] = Y[vi, vj] + A[vi, vk] + B[vk, vj] \n",
    "```\n",
    "```python\n",
    "# corressponding numpy code\n",
    "vi, vj, vk = i, j, k\n",
    "if vk == 0:\n",
    "    Y[vi, vj] = 0\n",
    "Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**块**是TensorIR中的基本计算单位。值得注意的是，该块包含比普通NumPy代码更多的信息。一个块包含一组块轴(vi、vj、vk)和围绕它们定义的计算"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "vi = T.axis.spatial(128, i)\n",
    "vj = T.axis.spatial(128, j)\n",
    "vk = T.axis.reduce(128, k)\n",
    "# [block_axis] = T.axis.[axis_type]([axis_range], [mapped_value])\n",
    "```\n",
    "声明块轴的关键性质: \n",
    "1. 定义vi, vj, vk被绑定到的位置(本例中的i, j, k)\n",
    "2. 声明vi, vj, vk的原始范围/预期范围(T.axis.spatial(128, i))\n",
    "3. 声明块轴的属性(spatial, reduce)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3.4 块轴的属性\n",
    "块Y通过读取来自A[vi, vk]和B[vk, vj]的值来计算结果Y[vi, vj]，并对所有可能的vk执行求和，对于一组固定的 vi 和 vj，计算块在Y的空间位置(Y[vi, vj])处生成一个点值，该点值独立于Y中的其他位置（具有不同的vi, vj值的位置）。 \\\n",
    "vi, vj -> 空间轴(spatial axis): 直接对应于块写入的缓冲区空间区域的开始\\\n",
    "vk -> 归约轴(reduce axis): 涉及归约op\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3.5 为什么块需要额外的附加信息\n",
    "使块轴独立于外部循环嵌套i, j, k, 同时帮助验证计算循环的正确性\n",
    "```python\n",
    "for i in range(127):\n",
    "    with T.block('c'):\n",
    "        vi = T.axis.spatial(128, i)\n",
    "        # error here due to iterator size mismatch\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3.6 块轴绑定语法糖\n",
    "\n",
    "```python\n",
    "# SSR means the properties of each axes are \"spatial\", \"spatial\", \"reduce\"\n",
    "vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModelWithAxisRemapSuger():\n",
    "    @T.prim_func\n",
    "    def mm_relu(A: T.Buffer[(128, 128), 'float32'],\n",
    "                B: T.Buffer[(128, 128), 'float32'],\n",
    "                C: T.Buffer[(128, 128), 'float32']):\n",
    "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
    "        Y = T.alloc_buffer((128, 128), dtype='float32')\n",
    "        for i, j, k in T.grid(128, 128, 128):\n",
    "            with T.block(\"Y\"):\n",
    "                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])            # axis remap suger\n",
    "                with T.init():\n",
    "                    Y[vi, vj] = T.float32(0)\n",
    "                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
    "        \n",
    "        for i, j in T.grid(128, 128):\n",
    "            with T.block(\"C\"):\n",
    "                vi, vj = T.axis.remap(\"SS\", [i, j])                    # axis remap suger\n",
    "                C[vi, vj] = max(Y[vi, vj], T.float32(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3.7 函数属性和装饰器"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
    "```\n",
    "global_symbol -> 函数名; tir.noalias -> 属性，指所有缓冲存储器不重叠\n",
    "\n",
    "```python\n",
    "# 用于表述对应部分的类型\n",
    "@tvm.script.ir_module\n",
    "@T.prim_func\n",
    "```\n",
    "\n",
    "```python\n",
    "# 表示MyModule是一个IRModule -> IRModule是MLC中保存张量函数集合的容器对象\n",
    "@tvm.script.ir_module\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tvm.ir.module.IRModule'> <class 'tvm.tir.function.PrimFunc'>\n",
      "@mm_relu = primfn(A_handle: handle, B_handle: handle, C_handle: handle) -> ()\n",
      "  attr = {\"tir.noalias\": True, \"global_symbol\": \"mm_relu\"}\n",
      "  buffers = {A: Buffer(A_1: Pointer(global float32), float32, [128, 128], []),\n",
      "             B: Buffer(B_1: Pointer(global float32), float32, [128, 128], []),\n",
      "             C: Buffer(C_1: Pointer(global float32), float32, [128, 128], [])}\n",
      "  buffer_map = {A_handle: A, B_handle: B, C_handle: C} {\n",
      "  block([], \"root\") {\n",
      "    tir.reads([])\n",
      "    tir.writes([])\n",
      "    Y = alloc_buffer(float32[128, 128])\n",
      "     {\n",
      "      for (i: int32, 0, 128) {\n",
      "        for (j: int32, 0, 128) {\n",
      "          for (k: int32, 0, 128) {\n",
      "            block([128, 128, tir.reduce_axis(0, 128)], \"Y\") as [vi, vj, vk] {\n",
      "              bind(vi, i)\n",
      "              bind(vj, j)\n",
      "              bind(vk, k)\n",
      "              tir.reads([A[vi, vk], B[vk, vj]])\n",
      "              tir.writes([Y[vi, vj]])\n",
      "              with init() {\n",
      "                Y[vi, vj] = 0f32\n",
      "              }\n",
      "              Y[vi, vj] = (Y[vi, vj] + (A[vi, vk]*B[vk, vj]))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      for (i_1: int32, 0, 128) {\n",
      "        for (j_1: int32, 0, 128) {\n",
      "          block([128, 128], \"C\") as [vi_1, vj_1] {\n",
      "            bind(vi_1, i_1)\n",
      "            bind(vj_1, j_1)\n",
      "            tir.reads([Y[vi_1, vj_1]])\n",
      "            tir.writes([C[vi_1, vj_1]])\n",
      "            C[vi_1, vj_1] = max(Y[vi_1, vj_1], 0f32)\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(MyModel), type(MyModel['mm_relu']))\n",
    "print(MyModel)\n",
    "# 一个IRModule可以包含多个张量函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df7b789d6764113f3eb4ff8e192e7912fbf893c46539f75332048503ce5ba603"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
