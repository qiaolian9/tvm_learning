{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 与机器学习框架整合 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import tir as T\n",
    "from tvm.script import relax as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import fx\n",
    "from torch.nn import functional as F\n",
    "import torchvision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 通过builder构建IRModule\n",
    "构建IRModule：\n",
    "1. TVMScript(小模型，手动定义)\n",
    "2. Builder(大模型)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 从张量表达式(TE)构建TensorIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = te.placeholder((128, 128), name='A', dtype='float32')\n",
    "B = te.placeholder((128, 128), name='B', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tvm.te.tensor.Tensor, Tensor(shape=[128, 128], op.name=A))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(A), A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te_matmul\n",
    "def te_matmul(A: te.Tensor, B: te.Tensor):\n",
    "    m = A.shape[0]\n",
    "    n = B.shape[1]\n",
    "    k = te.reduce_axis((0, A.shape[1]), name='k')\n",
    "    return te.compute((m, n), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name='matmul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.tir.function.PrimFunc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = te_matmul(A, B)\n",
    "type(te.create_prim_func([A, B, C]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/script/highlight.py:116: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "/staff/qiaoliang/anaconda3/envs/MLC/bin/python -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">func</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], matmul: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "    <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "    <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "    <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;matmul&quot;</span>):\n",
       "            v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[<span style=\"color: #008000\">128</span>, v_k], B[v_k, <span style=\"color: #008000\">128</span>])\n",
       "            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(matmul[v_i, v_j])\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "            matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[<span style=\"color: #008000\">128</span>, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_k, <span style=\"color: #008000\">128</span>]\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "te.create_prim_func([A, B, C]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def te_relu(A: te.Tensor):\n",
    "    return te.compute(A.shape, lambda *i: te.max(A(*i), 0), name='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/script/highlight.py:116: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "/staff/qiaoliang/anaconda3/envs/MLC/bin/python -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">func</span>(X1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[<span style=\"color: #008000\">10</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>], relu: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[<span style=\"color: #008000\">10</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "    <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "    <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "    <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>serial(<span style=\"color: #008000\">10</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "            v_i0 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">10</span>, i0)\n",
       "            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(X1[v_i0])\n",
       "            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(relu[v_i0])\n",
       "            relu[v_i0] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(X1[v_i0], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X1 = te.placeholder((10, ), name='X1', dtype='float32')\n",
    "Y1 = te_relu(X1)\n",
    "te.create_prim_func([X1, Y1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/script/highlight.py:116: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "/staff/qiaoliang/anaconda3/envs/MLC/bin/python -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">func</span>(X2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">20</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], relu: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">20</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "    <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "    <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "    <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">20</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "            v_i0, v_i1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(X2[v_i0, v_i1])\n",
       "            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "            relu[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(X2[v_i0, v_i1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X2 = te.placeholder((10, 20), name='X2', dtype='float32')\n",
    "Y2 = te_relu(X2)\n",
    "te.create_prim_func([X2, Y2]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "te API运行融合算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/script/highlight.py:116: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "/staff/qiaoliang/anaconda3/envs/MLC/bin/python -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">func</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], relu: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "    <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "    <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "    <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "    matmul <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer([<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>], dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;matmul&quot;</span>):\n",
       "            v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[<span style=\"color: #008000\">128</span>, v_k], B[v_k, <span style=\"color: #008000\">128</span>])\n",
       "            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(matmul[v_i, v_j])\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "            matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[<span style=\"color: #008000\">128</span>, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_k, <span style=\"color: #008000\">128</span>]\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "            v_i0, v_i1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(matmul[v_i0, v_i1])\n",
       "            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "            relu[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(matmul[v_i0, v_i1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C = te_matmul(A, B)\n",
    "D = te_relu(C)\n",
    "te.create_prim_func([A, B, D]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "te.create_prim_func([A, B, C, D]).show()\n",
    "```\n",
    "区别，显式传入C需要在参数输入时也对应传入，隐式传入则是由tir申请中间变量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 使用BlockBuilder构建IRModule(IRModule中包含多个TensorIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = relax.Var(\"A\", relax.TensorStructInfo((128, 128), 'float32'))\n",
    "B = relax.Var(\"B\", relax.TensorStructInfo((128, 128), 'float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/script/highlight.py:116: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "/staff/qiaoliang/anaconda3/envs/MLC/bin/python -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">te_matmul</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], matmul: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;matmul&quot;</span>):\n",
       "                v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), v_k], rxplaceholder_1[v_k, T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(matmul[v_i, v_j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> rxplaceholder[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[v_k, T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">te_relu</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], relu: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "                v_i0, v_i1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[v_i0, v_i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "                relu[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(rxplaceholder[v_i0, v_i1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(A: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(te_matmul, (A, B), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(te_relu, (lv,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "        \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bb = relax.BlockBuilder()\n",
    "with bb.function('main'):\n",
    "    with bb.dataflow():\n",
    "        C = bb.emit_te(te_matmul, A, B)\n",
    "        D = bb.emit_te(te_relu, C)\n",
    "        R = bb.emit_output(D)\n",
    "    bb.emit_func_output(R, params=[A, B])\n",
    "\n",
    "MyModule = bb.get()\n",
    "MyModule.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 深入理解blockbuilder API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlockBuilder 带有与 Relax 函数中相应的作用域。例如，bb.dataflow() 创建一个 dataflow block，其中所有对 BlockBuilder 的调用都处在 dataflow block 的作用域中\n",
    "\n",
    "```python\n",
    "with bb.function('main'):\n",
    "    with bb.dataflow():\n",
    "        # every emit call generates a variable inside a dataflow block.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.relax.expr.DataflowVar"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DatalowVar ---> 计算图中间值\n",
    "type(C)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax中函数每一行都由emit_te调用生成\n",
    "```python\n",
    "lv = R.call_tir(te_matmul, (A, B), relax.TensorStructureInfo((128, 128), dtype='float32))\n",
    "C = bb.emit_te(te_mmatmul, A, B)\n",
    "```\n",
    "\n",
    "在幕后，bb.emit_te 做了以下事情：\n",
    "1. 为 A 和 B 创建一个输入 te.placeholder。\n",
    "2. 通过 te_matmul 函数运行它们。\n",
    "3. 调用 te.create_prim_func 来创建一个 TensorIR 函数。\n",
    "4. 通过 call_tir 生成对函数的调用。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bb.emit_output()创建每个dataflow block的输出变量，即可在计算图之外引用的变量\n",
    "```python\n",
    "with bb.dataflow():\n",
    "    ...\n",
    "    R = bb.emmit_output(D)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，函数输出由 bb.emit_func_output 标记。 我们只能在每个函数作用域内调用一次 emit_func_output。\n",
    "```python\n",
    "with bb.function('main'):\n",
    "    ...\n",
    "    with bb.emit_func_output(R, params=[A, B])\n",
    "\n",
    "with bb.dunction('main', params=[A, B]):\n",
    "    ...\n",
    "    bb.emmit_func_output(R)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch ---> tvm.IRModule\n",
    "\n",
    "**key**: 计算图抽象 \n",
    "\n",
    "process: nn.Module(torch) ---> Computation graph(torch) ---> IRModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch version\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(128, 128))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        x = torch.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 创建TorchFX GraphModule\n",
    "\n",
    "使用TorchFX表示Pytorch模型计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel()\n",
    "fx_module = fx.symbolic_trace(model)\n",
    "type(fx_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name    target                                                     args         kwargs\n",
      "-------------  ------  ---------------------------------------------------------  -----------  --------\n",
      "placeholder    x       x                                                          ()           {}\n",
      "get_attr       weight  weight                                                     ()           {}\n",
      "call_function  matmul  <built-in method matmul of type object at 0x7fd936bffec0>  (x, weight)  {}\n",
      "call_function  relu    <built-in method relu of type object at 0x7fd936bffec0>    (matmul,)    {}\n",
      "output         output  output                                                     (relu,)      {}\n"
     ]
    }
   ],
   "source": [
    "fx_module.graph.print_tabular()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.2 构造映射函数\n",
    "让我们定义整体的翻译逻辑。 主要流程如下：\n",
    "1. 创建一个 node_map，将 fx.Node 映射到相应的 relax.Var，该 relax.Var 代表 IRModule 中的已翻译节点。\n",
    "2. 以拓扑顺序迭代 FX 图中的节点。\n",
    "3. 给定映射输入，获取节点的映射输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [#users=1] = placeholder[target=x]\n",
      "    %weight : [#users=1] = get_attr[target=weight]\n",
      "    %matmul : [#users=1] = call_function[target=torch.matmul](args = (%x, %weight), kwargs = {})\n",
      "    %relu : [#users=1] = call_function[target=torch.relu](args = (%matmul,), kwargs = {})\n",
      "    return relu \n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    weight = self.weight\n",
      "    matmul = torch.matmul(x, weight);  x = weight = None\n",
      "    relu = torch.relu(matmul);  matmul = None\n",
      "    return relu\n",
      "     <bound method GraphModule.__str__ of MyModel()>\n",
      "x <class 'torch.fx.node.Node'> placeholder x\n",
      "weight <class 'torch.fx.node.Node'> get_attr weight\n",
      "matmul <class 'torch.fx.node.Node'> call_function <built-in method matmul of type object at 0x7fd936bffec0>\n",
      "relu <class 'torch.fx.node.Node'> call_function <built-in method relu of type object at 0x7fd936bffec0>\n",
      "output <class 'torch.fx.node.Node'> output output\n"
     ]
    }
   ],
   "source": [
    "print(fx_module.graph, fx_module.code, fx_module.__str__)\n",
    "for node in fx_module.graph.nodes:\n",
    "    print(node, type(node), node.op, node.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_param(param: nn.Parameter):\n",
    "    ndim = param.data.shape\n",
    "    return relax.const(param.data.cpu().numpy(), 'float32')\n",
    "\n",
    "def fetch_attr(fx_mod, target: str):\n",
    "    target_atoms = target.split('.')\n",
    "    attr_itr = fx_mod\n",
    "    for i, atom in enumerate(target_atoms):\n",
    "        if not hasattr(attr_itr, atom):\n",
    "            raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n",
    "        attr_itr = getattr(attr_itr, atom)\n",
    "    return attr_itr\n",
    "\n",
    "def from_fx(fx_mod: torch.fx.GraphModule, input_shapes, call_function_map, call_module_map):\n",
    "    input_index = 0\n",
    "    node_map = {}\n",
    "    named_modules = dict(fx_mod.named_modules())\n",
    "\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    fn_inputs = []\n",
    "    fn_output = None\n",
    "    with bb.function('main'):\n",
    "        with bb.dataflow():\n",
    "            for node in fx_mod.graph.nodes:\n",
    "                if node.op == \"placeholder\":\n",
    "                    shape = input_shapes[input_index]\n",
    "                    input_index = input_index + 1\n",
    "                    input_var = relax.Var(node.target, relax.TensorStructInfo(shape, dtype='float32'))\n",
    "                    fn_inputs.append(input_var)\n",
    "                    node_map[node] = input_var\n",
    "                elif node.op == \"get_attr\":\n",
    "                    node_map[node] = map_param(fetch_attr(fx_mod, node.target))\n",
    "                elif node.op == \"call_function\":\n",
    "                    node_map[node] = call_function_map[node.target](bb, node_map, node)\n",
    "                elif node.op == \"call_module\":\n",
    "                    named_module = named_modules[node.target]\n",
    "                    node_map[node] = call_module_map[type(named_module)](bb, node_map, node, named_module)\n",
    "                elif node.op == \"output\":\n",
    "                    output = node_map[node.args[0]]\n",
    "                    assert fn_output is None\n",
    "                    fn_output = bb.emit_output(output)\n",
    "        \n",
    "        bb.emit_func_output(fn_output, fn_inputs)\n",
    "    return bb.get()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/script/highlight.py:116: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "/staff/qiaoliang/anaconda3/envs/MLC/bin/python -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">te_matmul</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], matmul: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;matmul&quot;</span>):\n",
       "                v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), v_k], rxplaceholder_1[v_k, T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(matmul[v_i, v_j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> matmul[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> rxplaceholder[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[v_k, T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">te_relu</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], relu: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "                v_i0, v_i1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[v_i0, v_i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "                relu[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(rxplaceholder[v_i0, v_i1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(te_matmul, (x, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(te_relu, (lv,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "        \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_matmul(bb, node_map, node):\n",
    "    A = node_map[node.args[0]]\n",
    "    W = node_map[node.args[1]]\n",
    "    return bb.emit_te(te_matmul, A, W)\n",
    "\n",
    "def map_relu(bb, node_map, node):\n",
    "    A = node_map[node.args[0]]\n",
    "    return bb.emit_te(te_relu, A)\n",
    "\n",
    "MyModule = from_fx(fx_module, [(1, 128)],\n",
    "                call_function_map={torch.matmul: map_matmul, torch.relu: map_relu},\n",
    "                call_module_map={})\n",
    "\n",
    "MyModule.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 FashionMNist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "img, label = next(iter(test_loader))\n",
    "img = img.reshape(1, 28, 28).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Class:\", class_names[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch version\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear0 = nn.Linear(784, 128, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(128, 10, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear0(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "mlp_model = MLP()\n",
    "mlp_params = pkl.load(open(\"fasionmnist_mlp_params.pkl\", \"rb\"))\n",
    "mlp_model.linear0.weight.data = torch.from_numpy(mlp_params['w0'])\n",
    "mlp_model.linear0.bias.data = torch.from_numpy(mlp_params['b0'])\n",
    "mlp_model.linear1.weight.data = torch.from_numpy(mlp_params['w1'])\n",
    "mlp_model.linear1.bias.data = torch.from_numpy(mlp_params['b1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Prediction: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "torch_res = mlp_model(torch.from_numpy(img.reshape(1, 784)))\n",
    "\n",
    "pred_kind = np.argmax(torch_res.detach().numpy(), axis=1)\n",
    "print(\"Torch Prediction:\", class_names[pred_kind[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/script/highlight.py:116: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "/staff/qiaoliang/anaconda3/envs/MLC/bin/python -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">add</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], T_add: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[v_ax0, v_ax1], rxplaceholder_1[v_ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1])\n",
       "                T_add[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> rxplaceholder[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> rxplaceholder_1[v_ax1]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">dense1</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], T_matmul_NT: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>], <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NT&quot;</span>):\n",
       "                v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[v_i, v_k], rxplaceholder_1[v_j, v_k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_matmul_NT[v_i, v_j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NT[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                T_matmul_NT[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NT[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> rxplaceholder[v_i, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[v_j, v_k]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">te_relu</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], relu: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "                v_i0, v_i1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[v_i0, v_i1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "                relu[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(rxplaceholder[v_i0, v_i1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">dense</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], T_matmul_NT: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>], <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NT&quot;</span>):\n",
       "                v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[v_i, v_k], rxplaceholder_1[v_j, v_k])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_matmul_NT[v_i, v_j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NT[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
       "                T_matmul_NT[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NT[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> rxplaceholder[v_i, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> rxplaceholder_1[v_j, v_k]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">add1</span>(rxplaceholder: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], rxplaceholder_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>], T_add: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer[(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>]):\n",
       "        <span style=\"color: #007979; font-style: italic\"># function attr dict</span>\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #007979; font-style: italic\"># body</span>\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(rxplaceholder[v_ax0, v_ax1], rxplaceholder_1[v_ax1])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1])\n",
       "                T_add[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> rxplaceholder[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> rxplaceholder_1[v_ax1]\n",
       "    \n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(dense, (x, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(add, (lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(te_relu, (lv1,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv3 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(dense1, (lv2, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv4 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(add1, (lv3, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv4\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "        \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 预定义库 TVM TOPI\n",
    "from tvm import topi\n",
    "def map_nn_linear(bb, node_map, node, nn_mod):\n",
    "    x = node_map[node.args[0]]\n",
    "    w = map_param(nn_mod.weight)\n",
    "    if nn_mod.bias is not None:\n",
    "        b = map_param(nn_mod.bias)\n",
    "    \n",
    "    y = bb.emit_te(topi.nn.dense, x, w)\n",
    "    return bb.emit_te(topi.add, y, b)\n",
    "\n",
    "def map_nn_relu(bb, node_map, node, nn_mode):\n",
    "    return map_relu(bb, node_map, node)\n",
    "\n",
    "MLPModule = from_fx(fx.symbolic_trace(mlp_model),\n",
    "                    input_shapes=[(1, 784)],\n",
    "                    call_function_map={},\n",
    "                    call_module_map={\n",
    "                        torch.nn.Linear: map_nn_linear,\n",
    "                        torch.nn.ReLU: map_nn_relu\n",
    "                    })\n",
    "\n",
    "MLPModule.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModule Prediction: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "ex = relax.vm.build(MLPModule, target='llvm')\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "\n",
    "data_nd = tvm.nd.array(img.reshape(1, 784))\n",
    "\n",
    "nd_res = vm['main'](data_nd)\n",
    "pred_kind = np.argmax(nd_res.numpy(), axis=1)\n",
    "print(\"MLPModule Prediction:\", class_names[pred_kind[0]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 翻译成高层次算子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/qiaoliang/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/script/highlight.py:116: UserWarning: No module named 'black'\n",
      "To print formatted TVM script, please install the formatter 'Black':\n",
      "/staff/qiaoliang/anaconda3/envs/MLC/bin/python -m pip install \"black==22.3.0\" --upgrade --user\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>], axes<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(x, lv, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv1, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
       "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv2)\n",
       "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>], axes<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            lv5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv3, lv4, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv5, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>])\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv6\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "        \n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_nn_relu_op(bb, node_map, node, nn_mod):\n",
    "    A = node_map[node.args[0]]\n",
    "    return bb.emit(relax.op.nn.relu(A))\n",
    "\n",
    "def map_nn_linear_op(bb, node_map, node, nn_mod):\n",
    "    x = node_map[node.args[0]]\n",
    "    w = map_param(nn_mod.weight)\n",
    "    b = None\n",
    "    if nn_mod.bias is not None:\n",
    "        b = map_param(nn_mod.bias)\n",
    "    return bb.emit(relax.op.linear(x, w, b))\n",
    "\n",
    "MLPModuleHighLevel = from_fx(\n",
    "    fx.symbolic_trace(mlp_model),\n",
    "    input_shape = [(1, 784)],\n",
    "    call_function_map={\n",
    "    },\n",
    "    call_module_map={\n",
    "        torch.nn.Linear: map_nn_linear_op,\n",
    "        torch.nn.ReLU: map_nn_relu_op,\n",
    "    },\n",
    ")\n",
    "\n",
    "MLPModuleHighLevel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  8: TVMFuncCall\n  7: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::relax::ExecBuilder, tvm::IRModule)>::AssignTypedLambda<tvm::IRModule (*)(tvm::relax::ExecBuilder, tvm::IRModule)>(tvm::IRModule (*)(tvm::relax::ExecBuilder, tvm::IRModule), std::string)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  6: tvm::relax::relax_vm::VMCodeGen(tvm::relax::ExecBuilder, tvm::IRModule)\n  5: tvm::relax::relax_vm::CodeGenVM::Run(tvm::relax::ExecBuilder, tvm::IRModule)\n  4: tvm::relax::relax_vm::CodeGenVM::Codegen(tvm::relax::Function const&)\n  3: tvm::relax::ExprFunctor<tvm::runtime::relax_vm::Instruction::Arg (tvm::RelayExpr const&)>::VisitExpr(tvm::RelayExpr const&)\n  2: tvm::relax::relax_vm::CodeGenVM::VisitExpr_(tvm::relax::SeqExprNode const*)\n  1: tvm::relax::ExprFunctor<tvm::runtime::relax_vm::Instruction::Arg (tvm::RelayExpr const&)>::VisitExpr(tvm::RelayExpr const&)\n  0: tvm::relax::relax_vm::CodeGenVM::VisitExpr_(tvm::relax::CallNode const*)\n  File \"/workspace/tvm/src/relax/backend/vm/codegen_vm.cc\", line 188\nTVMError: CodeGenVM cannot handle this intrinsic now:\nOp(relax.permute_dims)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/staff/qiaoliang/ACSA科研项目/tvm_learning/mlc/MLC#5/5#1.ipynb 单元格 47\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsnode2/staff/qiaoliang/ACSA%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE/tvm_learning/mlc/MLC%235/5%231.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ex \u001b[39m=\u001b[39m relax\u001b[39m.\u001b[39;49mvm\u001b[39m.\u001b[39;49mbuild(MLPModuleHighLevel, target\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mllvm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsnode2/staff/qiaoliang/ACSA%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE/tvm_learning/mlc/MLC%235/5%231.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m vm \u001b[39m=\u001b[39m relax\u001b[39m.\u001b[39mVirtualMachine(ex, tvm\u001b[39m.\u001b[39mcpu())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsnode2/staff/qiaoliang/ACSA%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE/tvm_learning/mlc/MLC%235/5%231.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m data_nd \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mnd\u001b[39m.\u001b[39marray(img\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m784\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/relax/vm.py:599\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(mod, target, params, exec_mode)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39m# builder collects the executable\u001b[39;00m\n\u001b[1;32m    598\u001b[0m builder \u001b[39m=\u001b[39m relax\u001b[39m.\u001b[39mExecBuilder()\n\u001b[0;32m--> 599\u001b[0m leftover_mod \u001b[39m=\u001b[39m _vmcodegen(builder, new_mod, exec_mode\u001b[39m=\u001b[39;49mexec_mode)\n\u001b[1;32m    600\u001b[0m tir_mod \u001b[39m=\u001b[39m _filter_tir(leftover_mod)\n\u001b[1;32m    601\u001b[0m \u001b[39mreturn\u001b[39;00m _vmlink(builder, target, tir_mod, ext_libs, params)\n",
      "File \u001b[0;32m~/anaconda3/envs/MLC/lib/python3.8/site-packages/tvm/relax/vm.py:478\u001b[0m, in \u001b[0;36m_vmcodegen\u001b[0;34m(builder, mod, exec_mode)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Running VM codegen.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \n\u001b[1;32m    460\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[39m    Left over IRModule that may contain extra functions.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m exec_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbytecode\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 478\u001b[0m     \u001b[39mreturn\u001b[39;00m _ffi_api\u001b[39m.\u001b[39;49mVMCodeGen(builder, mod)  \u001b[39m# type:ignore\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m exec_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcompiled\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    480\u001b[0m     \u001b[39mreturn\u001b[39;00m _ffi_api\u001b[39m.\u001b[39mVMTIRCodeGen(builder, mod)  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi:331\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.PackedFuncBase.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi:262\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi:251\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.FuncCall3\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtvm/_ffi/_cython/./base.pxi:181\u001b[0m, in \u001b[0;36mtvm._ffi._cy3.core.CHECK_CALL\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  8: TVMFuncCall\n  7: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::relax::ExecBuilder, tvm::IRModule)>::AssignTypedLambda<tvm::IRModule (*)(tvm::relax::ExecBuilder, tvm::IRModule)>(tvm::IRModule (*)(tvm::relax::ExecBuilder, tvm::IRModule), std::string)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  6: tvm::relax::relax_vm::VMCodeGen(tvm::relax::ExecBuilder, tvm::IRModule)\n  5: tvm::relax::relax_vm::CodeGenVM::Run(tvm::relax::ExecBuilder, tvm::IRModule)\n  4: tvm::relax::relax_vm::CodeGenVM::Codegen(tvm::relax::Function const&)\n  3: tvm::relax::ExprFunctor<tvm::runtime::relax_vm::Instruction::Arg (tvm::RelayExpr const&)>::VisitExpr(tvm::RelayExpr const&)\n  2: tvm::relax::relax_vm::CodeGenVM::VisitExpr_(tvm::relax::SeqExprNode const*)\n  1: tvm::relax::ExprFunctor<tvm::runtime::relax_vm::Instruction::Arg (tvm::RelayExpr const&)>::VisitExpr(tvm::RelayExpr const&)\n  0: tvm::relax::relax_vm::CodeGenVM::VisitExpr_(tvm::relax::CallNode const*)\n  File \"/workspace/tvm/src/relax/backend/vm/codegen_vm.cc\", line 188\nTVMError: CodeGenVM cannot handle this intrinsic now:\nOp(relax.permute_dims)"
     ]
    }
   ],
   "source": [
    "ex = relax.vm.build(MLPModuleHighLevel, target='llvm')\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "\n",
    "data_nd = tvm.nd.array(img.reshape(1, 784))\n",
    "\n",
    "nd_res = vm['main'](data_nd)\n",
    "pred_kind = np.argmax(nd_res.numpy(), axis=1)\n",
    "print(\"MLPModuleHighLevel Prediction:\", class_names[pred_kind[0]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 总结\n",
    "张量表达式 API 允许我们创建原始的 TensorIR 函数。\n",
    "\n",
    "BlockBuilder API 通过 emit_te 和其他函数创建 IRModule。\n",
    "\n",
    "通过将模型转换为 IRModule，实现与现有的机器学习框架的整合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df7b789d6764113f3eb4ff8e192e7912fbf893c46539f75332048503ce5ba603"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
