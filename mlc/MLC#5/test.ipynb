{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T\n",
    "from tvm.ir import IRModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fx as fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand((64, 10)))\n",
    "        self.bias = nn.Parameter(torch.rand((10, )))\n",
    "        # self.linear1 = nn.Linear(10, 3, bias=True)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.conv = nn.Conv2d(1, 1, 3, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.conv(x)\n",
    "        # x = torch.relu(x)\n",
    "        x = x.view((1, -1))\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        # x = torch.add(x, self.bias)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.linear1(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_mod = MyModule()\n",
    "hw = 8\n",
    "data = np.random.rand(1, 1, hw, hw).astype('float32')\n",
    "\n",
    "torch_out = torch_mod(torch.from_numpy(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def te_matmul(x: te.Tensor, w: te.Tensor):\n",
    "    n = x.shape[0]\n",
    "    m = w.shape[1]\n",
    "    k = te.reduce_axis((0, x.shape[1]), 'k')\n",
    "    return te.compute((n, m), lambda i, j: te.sum(x[i, k] * w[k, j], axis=k), name='matmul')\n",
    "\n",
    "def te_add(x: te.Tensor, b: te.Tensor):\n",
    "    n, m = x.shape\n",
    "    return te.compute(x.shape, lambda i, j: x[i, j] + b[j], name='add')\n",
    "\n",
    "def te_relu(x: te.Tensor):\n",
    "    return te.compute(x.shape, lambda *i: te.max(x(*i), 0), name='relu')\n",
    "\n",
    "def te_view(x: te.Tensor):\n",
    "    n, c, h, w = x.shape\n",
    "    m = np.prod(x.shape) // n\n",
    "    return te.compute((n, m), lambda i, j: x[i, j // (h * w), j % (h * w) // w, j % (h * w) % w], name='view')\n",
    "\n",
    "def te_conv2d(x: te.Tensor, kernel: te.Tensor):\n",
    "    n, ci, h, w = x.shape\n",
    "    _, co, k1, k2 = kernel.shape\n",
    "    h_1, w_1 = h - k1 + 1, w - k2 + 1\n",
    "    ci = te.reduce_axis((0, ci), name='ci')\n",
    "    k1 = te.reduce_axis((0, k1), name='k1')\n",
    "    k2 = te.reduce_axis((0, k2), name='k2')\n",
    "    return te.compute((n, co, h_1, w_1), lambda i, j, k, l:\n",
    "                            te.sum(x[i, ci, k+k1, l+k2] * kernel[co, ci, k1, k2], axis=[ci, k1, k2]), name='conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = te.placeholder((1,1,10,10), dtype='float32')\n",
    "# K = te.placeholder((1, 1, 3, 3), dtype='float32')\n",
    "# C = te_conv2d(A, K)\n",
    "# print(C.shape)\n",
    "# te.create_prim_func([A, K, C]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode       name    target    args          kwargs\n",
      "-----------  ------  --------  ------------  --------\n",
      "placeholder  x       x         ()            {}\n",
      "call_method  view    view      (x, (1, -1))  {}\n",
      "output       output  output    (view,)       {}\n"
     ]
    }
   ],
   "source": [
    "# create computation graph\n",
    "fx_module = fx.symbolic_trace(torch_mod)\n",
    "fx_module.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for node in fx_module.graph.nodes:\n",
    "#     print(node, type(node), type(node.target))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with bb.function('main):\n",
    "    with bb.dataflow():\n",
    "        y = bb.emit_te(func_name, input_)\n",
    "        ...\n",
    "        with bb.emit_output(out)\n",
    "    with bb.emit_func_output(out, fn_inputs)\n",
    "\n",
    "MyModule = bb.get() # ---> IRModule\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造映射函数\n",
    "def map_params(param: nn.Parameter):\n",
    "    return relax.const(param.data.cpu().numpy(), dtype='float32')\n",
    "\n",
    "def fetch_attr(fx_mod, target: str):\n",
    "    target_atoms = target.split('.')\n",
    "    attr_itr = fx_mod\n",
    "    for i, atom in enumerate(target_atoms):\n",
    "        if not hasattr(attr_itr, atom):\n",
    "            raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n",
    "        attr_itr = getattr(attr_itr, atom)\n",
    "    return attr_itr\n",
    "\n",
    "def from_fx(fx_module, input_shapes, call_function_map, call_module_map, call_method_map):\n",
    "    fn_inputs = []\n",
    "    fn_output = None\n",
    "    input_index = 0\n",
    "\n",
    "    node_map = {}\n",
    "    named_modules = dict(fx_module.named_modules())\n",
    "\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    with bb.function('main'):\n",
    "        with bb.dataflow():\n",
    "            for node in fx_module.graph.nodes:\n",
    "                if node.op == \"placeholder\":\n",
    "                    shape = input_shapes[input_index]\n",
    "                    input_index = input_index + 1\n",
    "                    fn_input = relax.Var(node.target, R.Tensor(shape=shape, dtype='float32'))\n",
    "                    fn_inputs.append(fn_input)\n",
    "                    node_map[node] = fn_input\n",
    "                elif node.op == \"get_attr\":\n",
    "                    node_map[node] = map_params(fetch_attr(fx_module, node.target))\n",
    "                elif node.op == \"call_function\":\n",
    "                    node_map[node] = call_function_map[node.target](bb, node_map, node)\n",
    "                elif node.op == \"call_module\":\n",
    "                    named_module = named_modules[node.target]\n",
    "                    node_map[node] = call_module_map[type(named_module)](bb, node_map, node, named_module)\n",
    "                elif node.op == \"call_method\":\n",
    "                    node_map[node] = call_method_map[node.target](bb, node_map, node)\n",
    "                elif node.op == \"output\":\n",
    "                    output = node_map[node.args[0]]\n",
    "                    assert fn_output is None\n",
    "                    fn_output = bb.emit_output(output)\n",
    "        bb.emit_func_output(fn_output, fn_inputs)\n",
    "    return bb.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_function\n",
    "def map_matmul(bb, node_map, node):\n",
    "    x = node_map[node.args[0]]\n",
    "    w = node_map[node.args[1]]\n",
    "    return bb.emit_te(te_matmul, x, w)\n",
    "\n",
    "def map_relu(bb, node_map, node):\n",
    "    x = node_map[node.args[0]]\n",
    "    return bb.emit_te(te_relu, x)\n",
    "\n",
    "def map_add(bb, node_map, node):\n",
    "    x = node_map[node.args[0]]\n",
    "    b = node_map[node.args[1]]\n",
    "    return bb.emit_te(te_add, x, b)\n",
    "\n",
    "# call_module\n",
    "from tvm import topi\n",
    "def map_nn_conv(bb, node_map, node, nn_mod):\n",
    "    x = node_map[node.args[0]]\n",
    "    kernel = map_params(nn_mod.weight)\n",
    "    return bb.emit_te(te_conv2d, x, kernel)\n",
    "\n",
    "def map_nn_linear(bb, node_map, node, nn_mod):\n",
    "    x = node_map[node.args[0]]\n",
    "    w = map_params(nn_mod.weight)\n",
    "    b = None\n",
    "    if nn_mod.bias is not None:\n",
    "        b = map_params(nn_mod.bias)\n",
    "    return bb.emit_te(topi.nn.dense, x, w, b)\n",
    "    \n",
    "def map_nn_relu(bb, node_map, node, nn_mod):\n",
    "    x = node_map[node.args[0]]\n",
    "    return bb.emit_te(topi.nn.relu, x)\n",
    "\n",
    "# call method\n",
    "def map_view(bb, node_map, node):\n",
    "    x = node_map[node.args[0]]\n",
    "    return bb.emit_te(te_view, x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyDemo = from_fx(fx_module=fx_module, input_shapes=[(1, 1, hw, hw),],\n",
    "                    call_function_map={\n",
    "                        torch.matmul: map_matmul,\n",
    "                        torch.add: map_add,\n",
    "                        torch.relu: map_relu\n",
    "                    },\n",
    "                    call_module_map={\n",
    "                        torch.nn.ReLU: map_nn_relu,\n",
    "                        torch.nn.Conv2d: map_nn_conv,\n",
    "                        torch.nn.Linear: map_nn_linear\n",
    "                    },\n",
    "                    call_method_map={'view': map_view})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyDemo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = relax.vm.build(MyDemo, target='llvm')\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "\n",
    "tvm_out = vm['main'](tvm.nd.array(data))\n",
    "np.testing.assert_equal(tvm_out.numpy(), torch_out.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df7b789d6764113f3eb4ff8e192e7912fbf893c46539f75332048503ce5ba603"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
